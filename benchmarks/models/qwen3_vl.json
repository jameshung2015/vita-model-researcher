[
  {
    "name": "MMMU_VAL",
    "metric": "Accuracy",
    "value": "78.7%",
    "description": "University-level comprehensive exam QA (multimodal questions):contentReference[oaicite:0]{index=0}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mmmu_val"
  },
  {
    "name": "MMMU_Pro",
    "metric": "Accuracy",
    "value": "68.1%",
    "description": "University-level comprehensive exam QA – advanced/professional level:contentReference[oaicite:1]{index=1}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mmmu_pro"
  },
  {
    "name": "MathVista_mini",
    "metric": "Accuracy",
    "value": "84.9%",
    "description": "Mathematics visual reasoning (mini version of MathVista dataset):contentReference[oaicite:2]{index=2}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mathvista_mini"
  },
  {
    "name": "MathVision",
    "metric": "Accuracy",
    "value": "66.5%",
    "description": "Visual math problem solving and causal reasoning:contentReference[oaicite:3]{index=3}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mathvision"
  },
  {
    "name": "MathVerse_mini",
    "metric": "Accuracy",
    "value": "85.0%",
    "description": "Mathematics puzzle solving (mini subset of MathVerse):contentReference[oaicite:4]{index=4}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mathverse_mini"
  },
  {
    "name": "ZeroBench",
    "metric": "Accuracy",
    "value": "29.9%",
    "description": "Visual logic puzzles and zero-shot reasoning challenge:contentReference[oaicite:5]{index=5}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.zerobench"
  },
  {
    "name": "VisionMagic_Sub",
    "metric": "Accuracy",
    "value": "89.9%",
    "description": "Visual magic puzzles (subset) – logical visual puzzle solving:contentReference[oaicite:6]{index=6}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.visionmagic_sub"
  },
  {
    "name": "RealWorldQA",
    "metric": "Accuracy",
    "value": "78.4%",
    "description": "General visual question answering on real-world images:contentReference[oaicite:7]{index=7}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.realworldqa"
  },
  {
    "name": "MMBench_EN_V1.1_dev",
    "metric": "Accuracy",
    "value": "90.6%",
    "description": "Comprehensive multimodal benchmark (English, v1.1 dev set):contentReference[oaicite:8]{index=8}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mmbench_en_v1_1_dev"
  },
  {
    "name": "MM-Situ",
    "metric": "Accuracy",
    "value": "78.4%",
    "description": "Multimodal situational understanding and VQA:contentReference[oaicite:9]{index=9}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mm_situ"
  },
  {
    "name": "SimplyVQA",
    "metric": "Accuracy",
    "value": "63.0%",
    "description": "Simple visual question answering tasks:contentReference[oaicite:10]{index=10}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.simplyvqa"
  },
  {
    "name": "HallusionBench",
    "metric": "Accuracy",
    "value": "63.2%",
    "description": "Visual hallucination benchmark – tests factual accuracy in image responses:contentReference[oaicite:11]{index=11}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.hallusionbench"
  },
  {
    "name": "MM_MT_Bench",
    "metric": "Score",
    "value": "91.3",
    "description": "Multimodal multi-turn instruction-following benchmark (chat quality evaluation):contentReference[oaicite:12]{index=12}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mm_mt_bench"
  },
  {
    "name": "MIA_Bench",
    "metric": "Score",
    "value": "8.5",
    "description": "Multimodal Instruction Alignment benchmark (measures alignment/following of instructions):contentReference[oaicite:13]{index=13}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mia_bench"
  },
  {
    "name": "MMLongBench-Doc",
    "metric": "Accuracy",
    "value": "57.0%",
    "description": "Long document understanding and QA (multimodal long context benchmark):contentReference[oaicite:14]{index=14}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mmlongbench_doc"
  },
  {
    "name": "DocVQA_TEST",
    "metric": "Accuracy",
    "value": "89.2%",
    "description": "Document image question answering (DocVQA dataset, test set):contentReference[oaicite:15]{index=15}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.docvqa_test"
  },
  {
    "name": "InfoVQA_TEST",
    "metric": "Accuracy",
    "value": "97.1%",
    "description": "Infographic image question answering (InfoVQA dataset, test set):contentReference[oaicite:16]{index=16}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.infovqa_test"
  },
  {
    "name": "AI2D_TEST",
    "metric": "Accuracy",
    "value": "89.7%",
    "description": "AI2D diagram understanding and QA (science diagrams dataset, test set):contentReference[oaicite:17]{index=17}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.ai2d_test"
  },
  {
    "name": "OCRBench (en/zh)",
    "metric": "Accuracy",
    "value": "67.1% / 61.8%",
    "description": "OCR text recognition accuracy (English/Chinese text):contentReference[oaicite:18]{index=18}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.ocrbench_en_zh"
  },
  {
    "name": "CC_OCR",
    "metric": "Accuracy",
    "value": "82.2%",
    "description": "Complex/OCR challenge – Chinese & English mixed text recognition:contentReference[oaicite:19]{index=19}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.cc_ocr"
  },
  {
    "name": "ChartX (RQ)",
    "metric": "Accuracy",
    "value": "62.1%",
    "description": "Chart interpretation (reasoning questions subset of chart QA):contentReference[oaicite:20]{index=20}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.chartx_rq"
  },
  {
    "name": "RefCOCO-avg",
    "metric": "Accuracy",
    "value": "91.9%",
    "description": "Referring expression object localization (average accuracy over RefCOCO datasets):contentReference[oaicite:21]{index=21}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.refcoco_avg"
  },
  {
    "name": "CountBench",
    "metric": "Accuracy",
    "value": "88.6%",
    "description": "Object counting in images (counting benchmark accuracy):contentReference[oaicite:22]{index=22}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.countbench"
  },
  {
    "name": "OdinW13",
    "metric": "Accuracy",
    "value": "53.9%",
    "description": "Visual detection/grounding in the wild (Odin-W13 challenge):contentReference[oaicite:23]{index=23}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.odinw13"
  },
  {
    "name": "ARKWebComs",
    "metric": "Accuracy",
    "value": "13.0%",
    "description": "3D scene understanding/grounding (AR Kit/AR web common scenes):contentReference[oaicite:24]{index=24}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.arkwebcoms"
  },
  {
    "name": "HyperSim",
    "metric": "Accuracy",
    "value": "39.4%",
    "description": "Synthetic 3D scene understanding (HyperSim dataset):contentReference[oaicite:25]{index=25}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.hypersim"
  },
  {
    "name": "SUNRGBD",
    "metric": "Accuracy",
    "value": "70.7%",
    "description": "Indoor 3D object detection and scene understanding (SUN RGB-D):contentReference[oaicite:26]{index=26}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.sunrgbd"
  },
  {
    "name": "Objectron",
    "metric": "Accuracy",
    "value": "71.2%",
    "description": "3D object pose estimation from video (Objectron dataset):contentReference[oaicite:27]{index=27}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.objectron"
  },
  {
    "name": "BLINK",
    "metric": "Accuracy",
    "value": "70.7%",
    "description": "Multi-image reasoning – cross-image inference tasks (BLINK benchmark):contentReference[oaicite:28]{index=28}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.blink"
  },
  {
    "name": "MUIRBENCH",
    "metric": "Accuracy",
    "value": "72.8%",
    "description": "Multi-image understanding benchmark (requires comparing multiple images):contentReference[oaicite:29]{index=29}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.muirbench"
  },
  {
    "name": "ERQA",
    "metric": "Accuracy",
    "value": "51.3%",
    "description": "Embodied visual question answering (agent-based visual QA):contentReference[oaicite:30]{index=30}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.erqa"
  },
  {
    "name": "VsiSpatialBench",
    "metric": "Accuracy",
    "value": "62.6%",
    "description": "Vision-spatial reasoning benchmark (tests spatial relationship understanding):contentReference[oaicite:31]{index=31}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.vsispatialbench"
  },
  {
    "name": "RefoSpatialBench",
    "metric": "Accuracy",
    "value": "83.1%",
    "description": "Referring & spatial understanding benchmark (spatial language comprehension):contentReference[oaicite:32]{index=32}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.refospatialbench"
  },
  {
    "name": "RoboSpatialHome",
    "metric": "Accuracy",
    "value": "69.5%",
    "description": "Embodied spatial reasoning in home/robot environment:contentReference[oaicite:33]{index=33}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.robospatialhome"
  },
  {
    "name": "VideoMME (w/o sub)",
    "metric": "Accuracy",
    "value": "79.2%",
    "description": "Video question answering (without subtitles provided):contentReference[oaicite:34]{index=34}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.videomme_w_o_sub"
  },
  {
    "name": "iLVBench",
    "metric": "Accuracy",
    "value": "84.3%",
    "description": "Long video understanding benchmark (evaluation on extended video content):contentReference[oaicite:35]{index=35}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.ilvbench"
  },
  {
    "name": "CharadesSTA",
    "metric": "Accuracy",
    "value": "67.7%",
    "description": "Temporal action localization in videos (Charades-STA dataset):contentReference[oaicite:36]{index=36}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.charadessta"
  },
  {
    "name": "VideoMMMU",
    "metric": "Accuracy",
    "value": "64.8%",
    "description": "Video-based multimodal knowledge exam (video version of MMMU):contentReference[oaicite:37]{index=37}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.videommmu"
  },
  {
    "name": "ScreenSpot",
    "metric": "Success rate",
    "value": "74.7%",
    "description": "GUI control task – locating and interacting with on-screen elements:contentReference[oaicite:38]{index=38}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.screenspot"
  },
  {
    "name": "ScreenSpot Pro",
    "metric": "Success rate",
    "value": "95.4%",
    "description": "Advanced GUI control tasks (more complex interface interactions):contentReference[oaicite:39]{index=39}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.screenspot_pro"
  },
  {
    "name": "OSWorldG",
    "metric": "Success rate",
    "value": "62.0%",
    "description": "Operating system control tasks (desktop environment automation):contentReference[oaicite:40]{index=40}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.osworldg"
  },
  {
    "name": "AndroidWorld",
    "metric": "Success rate",
    "value": "63.7%",
    "description": "Smartphone (Android) control tasks – mobile UI automation:contentReference[oaicite:41]{index=41}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.androidworld"
  },
  {
    "name": "Design2Code",
    "metric": "Accuracy",
    "value": "92.0%",
    "description": "Image-to-code generation (convert UI design image to HTML/CSS/JS code):contentReference[oaicite:42]{index=42}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.design2code"
  },
  {
    "name": "CharMimir_V2_Direct",
    "metric": "Accuracy",
    "value": "80.5%",
    "description": "Chart image to code generation (direct chart replication from image):contentReference[oaicite:43]{index=43}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.charmimir_v2_direct"
  },
  {
    "name": "UniSVG",
    "metric": "Accuracy",
    "value": "69.3%",
    "description": "Universal SVG generation – convert image to SVG vector graphics code:contentReference[oaicite:44]{index=44}",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.unisvg"
  }
]