{
  "model_info": {
    "model_name": "Qwen3-VL",
    "model_family": "Qwen3",
    "model_type": "vision-language",
    "variants": [
      {
        "name": "Qwen3-VL-4B-Instruct",
        "params": "4B",
        "release_date": "2025-10-15",
        "context_length": "256K (expandable to 1M)",
        "license": "Apache 2.0",
        "model_card": "https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct"
      },
      {
        "name": "Qwen3-VL-4B-Thinking",
        "params": "4B",
        "release_date": "2025-10-15",
        "context_length": "256K (expandable to 1M)",
        "license": "Apache 2.0",
        "model_card": "https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking"
      },
      {
        "name": "Qwen3-VL-8B-Instruct",
        "params": "8.77B",
        "release_date": "2025-10-15",
        "context_length": "256K (expandable to 1M)",
        "license": "Apache 2.0",
        "model_card": "https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct",
        "arena_elo": {
          "overall": 1427,
          "coding": 1457,
          "vision": 1246
        }
      },
      {
        "name": "Qwen3-VL-8B-Thinking",
        "params": "8.77B",
        "release_date": "2025-10-15",
        "context_length": "256K (expandable to 1M)",
        "license": "Apache 2.0",
        "model_card": "https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking"
      },
      {
        "name": "Qwen3-VL-30B-A3B-Instruct",
        "params": "30B (3B active)",
        "release_date": "2025-10-04",
        "context_length": "256K (expandable to 1M)",
        "license": "Apache 2.0",
        "model_card": "https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct"
      },
      {
        "name": "Qwen3-VL-30B-A3B-Thinking",
        "params": "30B (3B active)",
        "release_date": "2025-10-04",
        "context_length": "256K (expandable to 1M)",
        "license": "Apache 2.0",
        "model_card": "https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking"
      },
      {
        "name": "Qwen3-VL-235B-A22B-Instruct",
        "params": "235B (22B active)",
        "release_date": "2025-09-23",
        "context_length": "256K (expandable to 1M)",
        "license": "Apache 2.0",
        "model_card": "https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct",
        "arena_elo": {
          "overall": 1427,
          "coding": 1457,
          "vision": 1246
        }
      },
      {
        "name": "Qwen3-VL-235B-A22B-Thinking",
        "params": "235B (22B active)",
        "release_date": "2025-09-23",
        "context_length": "256K (expandable to 1M)",
        "license": "Apache 2.0",
        "model_card": "https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking",
        "arena_elo": {
          "overall": 1411,
          "coding": 1432,
          "vision": 1215
        }
      }
    ],
    "key_features": [
      "Visual agent operations (PC/mobile GUI interaction)",
      "Visual coding (Draw.io/HTML/CSS/JS generation)",
      "Advanced spatial perception with 2D/3D grounding",
      "Long video understanding with second-level indexing",
      "Enhanced multimodal reasoning for STEM/Math",
      "32 language OCR support (expanded from 19)",
      "Tool calling and structured output support"
    ],
    "architecture_innovations": [
      "Interleaved-MRoPE: Full-frequency allocation over time, width, height",
      "DeepStack: Multi-level ViT feature fusion for fine-grained details",
      "Text-Timestamp Alignment: Precise event localization for video temporal modeling"
    ],
    "last_updated": "2025-10-15"
  },
  "benchmark_results": [
  {
    "name": "MMMU_VAL",
    "metric": "Accuracy",
    "value": "78.7%",
    "description": "University-level comprehensive exam QA (multimodal questions)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mmmu_val"
  },
  {
    "name": "MMMU_Pro",
    "metric": "Accuracy",
    "value": "68.1%",
    "description": "University-level comprehensive exam QA – advanced/professional level",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mmmu_pro"
  },
  {
    "name": "MathVista_mini",
    "metric": "Accuracy",
    "value": "84.9%",
    "description": "Mathematics visual reasoning (mini version of MathVista dataset)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mathvista_mini"
  },
  {
    "name": "MathVision",
    "metric": "Accuracy",
    "value": "66.5%",
    "description": "Visual math problem solving and causal reasoning",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mathvision"
  },
  {
    "name": "MathVerse_mini",
    "metric": "Accuracy",
    "value": "85.0%",
    "description": "Mathematics puzzle solving (mini subset of MathVerse)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mathverse_mini"
  },
  {
    "name": "ZeroBench",
    "metric": "Accuracy",
    "value": "29.9%",
    "description": "Visual logic puzzles and zero-shot reasoning challenge",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.zerobench"
  },
  {
    "name": "VisionMagic_Sub",
    "metric": "Accuracy",
    "value": "89.9%",
    "description": "Visual magic puzzles (subset) – logical visual puzzle solving",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.visionmagic_sub"
  },
  {
    "name": "RealWorldQA",
    "metric": "Accuracy",
    "value": "78.4%",
    "description": "General visual question answering on real-world images",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.realworldqa"
  },
  {
    "name": "MMBench_EN_V1.1_dev",
    "metric": "Accuracy",
    "value": "90.6%",
    "description": "Comprehensive multimodal benchmark (English, v1.1 dev set)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mmbench_en_v1_1_dev"
  },
  {
    "name": "MM-Situ",
    "metric": "Accuracy",
    "value": "78.4%",
    "description": "Multimodal situational understanding and VQA",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mm_situ"
  },
  {
    "name": "SimplyVQA",
    "metric": "Accuracy",
    "value": "63.0%",
    "description": "Simple visual question answering tasks",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.simplyvqa"
  },
  {
    "name": "HallusionBench",
    "metric": "Accuracy",
    "value": "63.2%",
    "description": "Visual hallucination benchmark – tests factual accuracy in image responses",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.hallusionbench"
  },
  {
    "name": "MM_MT_Bench",
    "metric": "Score",
    "value": "91.3",
    "description": "Multimodal multi-turn instruction-following benchmark (chat quality evaluation)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mm_mt_bench"
  },
  {
    "name": "MIA_Bench",
    "metric": "Score",
    "value": "8.5",
    "description": "Multimodal Instruction Alignment benchmark (measures alignment/following of instructions)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mia_bench"
  },
  {
    "name": "MMLongBench-Doc",
    "metric": "Accuracy",
    "value": "57.0%",
    "description": "Long document understanding and QA (multimodal long context benchmark)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.mmlongbench_doc"
  },
  {
    "name": "DocVQA_TEST",
    "metric": "Accuracy",
    "value": "89.2%",
    "description": "Document image question answering (DocVQA dataset, test set)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.docvqa_test"
  },
  {
    "name": "InfoVQA_TEST",
    "metric": "Accuracy",
    "value": "97.1%",
    "description": "Infographic image question answering (InfoVQA dataset, test set)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.infovqa_test"
  },
  {
    "name": "AI2D_TEST",
    "metric": "Accuracy",
    "value": "89.7%",
    "description": "AI2D diagram understanding and QA (science diagrams dataset, test set)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.ai2d_test"
  },
  {
    "name": "OCRBench (en/zh)",
    "metric": "Accuracy",
    "value": "67.1% / 61.8%",
    "description": "OCR text recognition accuracy (English/Chinese text)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.ocrbench_en_zh"
  },
  {
    "name": "CC_OCR",
    "metric": "Accuracy",
    "value": "82.2%",
    "description": "Complex/OCR challenge – Chinese & English mixed text recognition",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.cc_ocr"
  },
  {
    "name": "ChartX (RQ)",
    "metric": "Accuracy",
    "value": "62.1%",
    "description": "Chart interpretation (reasoning questions subset of chart QA)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.chartx_rq"
  },
  {
    "name": "RefCOCO-avg",
    "metric": "Accuracy",
    "value": "91.9%",
    "description": "Referring expression object localization (average accuracy over RefCOCO datasets)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.refcoco_avg"
  },
  {
    "name": "CountBench",
    "metric": "Accuracy",
    "value": "88.6%",
    "description": "Object counting in images (counting benchmark accuracy)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.countbench"
  },
  {
    "name": "OdinW13",
    "metric": "Accuracy",
    "value": "53.9%",
    "description": "Visual detection/grounding in the wild (Odin-W13 challenge)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.odinw13"
  },
  {
    "name": "ARKWebComs",
    "metric": "Accuracy",
    "value": "13.0%",
    "description": "3D scene understanding/grounding (AR Kit/AR web common scenes)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.arkwebcoms"
  },
  {
    "name": "HyperSim",
    "metric": "Accuracy",
    "value": "39.4%",
    "description": "Synthetic 3D scene understanding (HyperSim dataset)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.hypersim"
  },
  {
    "name": "SUNRGBD",
    "metric": "Accuracy",
    "value": "70.7%",
    "description": "Indoor 3D object detection and scene understanding (SUN RGB-D)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.sunrgbd"
  },
  {
    "name": "Objectron",
    "metric": "Accuracy",
    "value": "71.2%",
    "description": "3D object pose estimation from video (Objectron dataset)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.objectron"
  },
  {
    "name": "BLINK",
    "metric": "Accuracy",
    "value": "70.7%",
    "description": "Multi-image reasoning – cross-image inference tasks (BLINK benchmark)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.blink"
  },
  {
    "name": "MUIRBENCH",
    "metric": "Accuracy",
    "value": "72.8%",
    "description": "Multi-image understanding benchmark (requires comparing multiple images)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.muirbench"
  },
  {
    "name": "ERQA",
    "metric": "Accuracy",
    "value": "51.3%",
    "description": "Embodied visual question answering (agent-based visual QA)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.erqa"
  },
  {
    "name": "VsiSpatialBench",
    "metric": "Accuracy",
    "value": "62.6%",
    "description": "Vision-spatial reasoning benchmark (tests spatial relationship understanding)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.vsispatialbench"
  },
  {
    "name": "RefoSpatialBench",
    "metric": "Accuracy",
    "value": "83.1%",
    "description": "Referring & spatial understanding benchmark (spatial language comprehension)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.refospatialbench"
  },
  {
    "name": "RoboSpatialHome",
    "metric": "Accuracy",
    "value": "69.5%",
    "description": "Embodied spatial reasoning in home/robot environment",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.robospatialhome"
  },
  {
    "name": "VideoMME (w/o sub)",
    "metric": "Accuracy",
    "value": "79.2%",
    "description": "Video question answering (without subtitles provided)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.videomme_w_o_sub"
  },
  {
    "name": "iLVBench",
    "metric": "Accuracy",
    "value": "84.3%",
    "description": "Long video understanding benchmark (evaluation on extended video content)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.ilvbench"
  },
  {
    "name": "CharadesSTA",
    "metric": "Accuracy",
    "value": "67.7%",
    "description": "Temporal action localization in videos (Charades-STA dataset)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.charadessta"
  },
  {
    "name": "VideoMMMU",
    "metric": "Accuracy",
    "value": "64.8%",
    "description": "Video-based multimodal knowledge exam (video version of MMMU)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.videommmu"
  },
  {
    "name": "ScreenSpot",
    "metric": "Success rate",
    "value": "74.7%",
    "description": "GUI control task – locating and interacting with on-screen elements",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.screenspot"
  },
  {
    "name": "ScreenSpot Pro",
    "metric": "Success rate",
    "value": "95.4%",
    "description": "Advanced GUI control tasks (more complex interface interactions)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.screenspot_pro"
  },
  {
    "name": "OSWorldG",
    "metric": "Success rate",
    "value": "62.0%",
    "description": "Operating system control tasks (desktop environment automation)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.osworldg"
  },
  {
    "name": "AndroidWorld",
    "metric": "Success rate",
    "value": "63.7%",
    "description": "Smartphone (Android) control tasks – mobile UI automation",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.androidworld"
  },
  {
    "name": "Design2Code",
    "metric": "Accuracy",
    "value": "92.0%",
    "description": "Image-to-code generation (convert UI design image to HTML/CSS/JS code)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.design2code"
  },
  {
    "name": "CharMimir_V2_Direct",
    "metric": "Accuracy",
    "value": "80.5%",
    "description": "Chart image to code generation (direct chart replication from image)",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.charmimir_v2_direct"
  },
  {
    "name": "UniSVG",
    "metric": "Accuracy",
    "value": "69.3%",
    "description": "Universal SVG generation – convert image to SVG vector graphics code",
    "source_type": "blog",
    "source": "DataLearner (Qwen3-VL results summary)",
    "link": "https://www.datalearner.com/blog/1051758672692718",
    "benchmark_id": "benchmark.unisvg"
  }
],
  "notes": {
    "data_source": "Primary benchmark results from DataLearner blog (https://www.datalearner.com/blog/1051758672692718)",
    "model_tested": "Qwen3-VL-235B-A22B-Instruct (primary)",
    "additional_sources": [
      "Chatbot Arena Leaderboard - Elo ratings for Instruct and Thinking variants",
      "HuggingFace model cards - Architecture and specifications",
      "Qwen technical documentation - Model capabilities and features"
    ],
    "performance_highlights": [
      "State-of-the-art on MathVision, Design2Code, and multiple text recognition benchmarks",
      "Outperforms Gemini 2.5 Pro on ScreenSpot Pro, OSWorldG, AndroidWorld, MMLongBench-Doc, DocVQATest, CountBench",
      "Second place behind Gemini Pro 2.5 on MMMU-Pro, SimpleVQA, VideoMMMU",
      "Matches or exceeds Gemini 2.5 Pro across major vision benchmarks (Alibaba reported)"
    ],
    "recommended_for": [
      "Automotive HMI understanding and GUI automation",
      "Document intelligence and OCR tasks",
      "Long video analysis with temporal grounding",
      "Spatial reasoning for embodied AI applications",
      "Multimodal STEM/Math problem solving"
    ]
  }
}
