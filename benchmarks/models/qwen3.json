{
  "model": "qwen3",
  "notes": "汇总公开报道与社区跑分，包含官方 blog/arXiv、代码仓、媒体报道与社区复现（注明来源类型）。",
  "benchmarks": [
    {
      "name": "MMLU / MMLU-Pro",
      "metric": "accuracy",
      "value": "~82-84 for large variants (reported)",
      "source_type": "community / paper / blog",
      "source": "Reddit community run (user report) and arXiv/blog summaries",
      "link": "https://www.reddit.com/r/LocalLLaMA/comments/1kh6kh3/qwen3_mmlupro_computer_science_llm_benchmark/"
    },
    {
      "name": "Various benchmarks (math/coding/reasoning)",
      "metric": "mixed (accuracy/Pass@1/other)",
      "value": "Competitive with top-tier models per official blog and press coverage (see notes)",
      "source_type": "official blog / press",
      "source": "Qwen team blog and press articles",
      "link": "https://qwenlm.github.io/blog/qwen3/"
    },
    {
      "name": "Repo / artifacts",
      "metric": "n/a",
      "value": "model cards, releases and benchmark config available",
      "source_type": "code repo",
      "source": "GitHub QwenLM/Qwen3",
      "link": "https://github.com/QwenLM/Qwen3"
    }
  ]
  ,
  "inference": {
    "notes": "Representative inference/deployment guidance synthesized from model card and community sources.",
    "variants": {
      "Qwen3-4B": {"latency_ms": null, "latency_level": "low", "throughput_rps": null, "memory_gb": 8, "quantization_friendly": true, "supported_hardware": ["CPU","small GPU"], "notes": "Estimate: FP16 ~8-12GB, INT8 ~4-6GB"},
      "Qwen3-8B": {"latency_ms": null, "latency_level": "medium", "throughput_rps": null, "memory_gb": 20, "quantization_friendly": true, "supported_hardware": ["A100","H100","large GPU"], "notes": "Estimate: FP16 ~16-24GB, INT8 ~8-12GB"},
      "Qwen3-30B-A3B": {"latency_ms": null, "latency_level": "high", "throughput_rps": null, "memory_gb": 64, "quantization_friendly": true, "supported_hardware": ["multi-GPU","cloud"], "notes": "MoE: activated params reduce memory but require MoE runtime; FP16 estimate ~48-80GB"},
      "Qwen3-235B-A22B": {"latency_ms": null, "latency_level": "very-high", "throughput_rps": null, "memory_gb": 400, "quantization_friendly": true, "supported_hardware": ["multi-node GPU clusters","cloud"], "notes": "Large MoE: total memory highly variable; use cloud hosted inference or multi-node clusters"}
    }
  }
}
