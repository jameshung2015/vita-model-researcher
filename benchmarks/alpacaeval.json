{
  "id": "benchmark.alpacaeval",
  "name": "AlpacaEval",
  "description": "自动化的指令跟随语言模型评估工具，专门设计用于快速、便宜且高度与人类评估相关的模型评测。AlpacaEval 2.0版本与ChatBot Arena的相关性达到0.98。",
  "source": "Stanford Tatsu Lab, 2023",
  "provider": "Stanford University",
  "category": "instruction_following",
  "datasets": [
    {
      "name": "AlpacaEval数据集",
      "description": "805个指令跟随评估样例，简化自AlpacaFarm评估集",
      "link": "https://tatsu-lab.github.io/alpaca_eval/",
      "huggingface": "https://huggingface.co/datasets/tatsu-lab/alpaca_eval",
      "github": "https://github.com/tatsu-lab/alpaca_eval",
      "license": "Apache-2.0"
    },
    {
      "name": "人类偏好数据",
      "description": "20K人类偏好标注，2.5K交叉标注",
      "link": "https://huggingface.co/datasets/tatsu-lab/alpaca_eval",
      "license": "Apache-2.0"
    }
  ],
  "data_structure": {
    "total_samples": "805个指令跟随评估样例",
    "source": "基于AlpacaFarm生成的数据（AlpacaEval来自AlpacaFarm evaluation set）",
    "format": "instruction (用户要求) + input (可选上下文) + model_response / reference answer",
    "evaluator": {
      "alpaca_eval_2.0": "使用gpt4_turbo作为annotator + baseline",
      "alpaca_eval_original": "使用alpaca_eval_gpt4作为annotator + text-davinci-003作为baseline"
    },
    "evaluation_method": "自动评估 (auto-annotator)：使用GPT-4（或其他LLM）评判模型与参考回答的胜率 (win rate)"
  },
  "data_examples": [
    {
      "example": "instruction: '请帮我写一封邮件给客户，内容是解释我们产品的优势并邀请对方参加下周的演示。' input: 一些上下文（比如产品特性、客户背景）。模型生成回应（邮件文本），然后用评判模型（如GPT-4）比较该回应与baseline回应并判断哪一个更好/是否满足指令。"
    }
  ],
  "evaluation_dimensions": [
    "指令跟随能力",
    "回答质量",
    "有用性",
    "准确性",
    "语言流畅性",
    "对话/指令完成能力",
    "LLM-as-judge评估效率",
    "与人类评估的相关性 (0.98 with ChatBot Arena)"
  ],
  "metrics": [
    {
      "name": "胜率",
      "definition": "模型输出相对于参考模型的胜率",
      "calculation": "wins / (wins + losses)"
    },
    {
      "name": "长度控制胜率",
      "definition": "控制长度偏差后的胜率",
      "calculation": "length_controlled_win_rate"
    },
    {
      "name": "标准误差",
      "definition": "胜率的统计标准误差",
      "calculation": "standard_error_of_mean"
    },
    {
      "name": "样本数",
      "definition": "用于计算胜率的样本总数",
      "calculation": "total_comparisons"
    }
  ],
  "test_method": "使用强大的LLM（如GPT-4）作为自动评估器，比较目标模型与参考模型的输出质量。支持缓存、批处理和输出随机化。默认使用weighted_alpaca_eval_gpt4_turbo评估器。",
  "technical_features": [
    "自动评估器",
    "长度偏差控制",
    "结果缓存",
    "批量处理",
    "输出随机化",
    "多评估器支持"
  ],
  "supported_evaluators": [
    "weighted_alpaca_eval_gpt4_turbo (推荐)",
    "alpaca_eval_gpt4",
    "alpaca_eval_cot_gpt4_turbo_fn",
    "claude",
    "其他自定义评估器"
  ],
  "example_scores": {
    "gpt4": "95.3% 胜率",
    "claude": "88.4% 胜率",
    "chatgpt": "86.1% 胜率",
    "guanaco-65b": "71.8% 胜率",
    "text_davinci_003": "50.0% 胜率 (基准)"
  },
  "cost_and_time": {
    "cost": "<$10 OpenAI credits",
    "time": "<3 minutes",
    "correlation": "0.98 with ChatBot Arena"
  },
  "installation": "pip install alpaca-eval",
  "usage": "alpaca_eval --model_outputs outputs.json --annotators_config weighted_alpaca_eval_gpt4_turbo",
  "limitations": [
    "指令可能不代表真实使用",
    "自动评估器存在偏差",
    "缺少安全性评估",
    "偏好长输出和列表格式"
  ],
  "website": "https://tatsu-lab.github.io/alpaca_eval/",
  "github": "https://github.com/tatsu-lab/alpaca_eval",
  "notes": "快速且成本低廉的指令跟随能力评估工具，与人类偏好高度相关。长度控制版本有效减少了长度偏差问题。适合模型开发阶段的快速评估。",
  "research_doc": "agents-toolchain/doc-eval-system/alpacaeval_research.md",
  "last_reviewed": "2025-10-13"
}