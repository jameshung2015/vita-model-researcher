{
  "id": "benchmark.urobench_en",
  "name": "URO-Bench (English)",
  "description": "English spoken dialogue evaluation benchmark testing understanding, reasoning, and generation capabilities in real-time audio conversations.",
  "source": "Step-Audio 2 Technical Report",
  "datasets": [
    {
      "name": "URO-Bench English",
      "link": "https://arxiv.org/html/2507.16632v1",
      "license": "Research use"
    }
  ],
  "metrics": [
    {
      "name": "score",
      "definition": "Overall URO-Bench score measuring audio understanding, reasoning, and output quality",
      "calculation": "Composite score across multiple audio task dimensions"
    }
  ],
  "test_method": "Evaluate models on structured English spoken dialogue tasks covering understanding, reasoning, and generation. Models process audio input and generate appropriate responses.",
  "example_scores": {
    "GPT-4o-audio": "84.54 (Step-Audio 2 Technical Report)",
    "Qwen-Omni": "70.58 (Step-Audio 2 Technical Report)"
  },
  "notes": "URO-Bench evaluates end-to-end audio dialogue capabilities. Scores reflect model performance on understanding spoken input, reasoning about content, and generating appropriate audio/text outputs."
}
