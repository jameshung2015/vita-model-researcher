{
  "id": "benchmark.lv_eval",
  "name": "LV-Eval",
  "description": "LV-Eval (Long-Context Validation Evaluation) 是一个专门评估长上下文语言模型能力的基准测试。它包含五大类任务：单跳QA、多跳QA、摘要、少样本学习和代码补全，涵盖11个双语（中英文）数据集，测试长度从3k到超过1M tokens。",
  "source": "Yuan et al., Tsinghua University, 2024",
  "provider": "Tsinghua University",
  "category": "long_context",
  "datasets": [
    {
      "name": "LV-Eval",
      "description": "包含5大类任务的11个双语数据集，覆盖3k-1M+ tokens长度范围",
      "link": "https://github.com/infinigence/LVEval",
      "license": "Apache-2.0"
    }
  ],
  "data_structure": {
    "total_tasks": "5大类任务",
    "task_categories": [
      "单跳QA (Single-hop QA)",
      "多跳QA (Multi-hop QA)",
      "摘要 (Summarization)",
      "少样本学习 (Few-shot Learning)",
      "代码补全 (Code Completion)"
    ],
    "datasets_count": "11个双语数据集",
    "context_lengths": "3k - 1M+ tokens",
    "languages": ["中文", "English"]
  },
  "evaluation_dimensions": [
    "长上下文理解 (long-context understanding)",
    "信息检索与定位 (information retrieval)",
    "跨文档推理 (cross-document reasoning)",
    "长文本摘要 (long-text summarization)",
    "上下文学习能力 (in-context learning)",
    "代码理解能力 (code understanding)"
  ],
  "metrics": [
    {
      "name": "accuracy",
      "definition": "QA任务的答案准确率",
      "calculation": "correct_answers / total_questions"
    },
    {
      "name": "rouge_score",
      "definition": "摘要任务的ROUGE评分",
      "calculation": "ROUGE-1/ROUGE-2/ROUGE-L scores"
    },
    {
      "name": "f1_score",
      "definition": "多跳QA的F1分数",
      "calculation": "2 * (precision * recall) / (precision + recall)"
    },
    {
      "name": "pass_rate",
      "definition": "代码补全任务的通过率",
      "calculation": "passed_tests / total_tests"
    }
  ],
  "test_method": "使用官方评估脚本，按任务类型分别评估。对于QA任务使用精确匹配或F1；摘要使用ROUGE；代码补全使用执行通过率。需要支持长上下文窗口（最长1M+ tokens）的模型推理框架。",
  "key_features": [
    "超长上下文支持 (up to 1M+ tokens)",
    "多任务多场景覆盖",
    "双语评估（中英文）",
    "真实场景任务设计",
    "分层难度设计",
    "可扩展的上下文长度"
  ],
  "example_scores": {
    "GPT-4-Turbo-128k": "~85.5 (paper, average across tasks)",
    "Claude-2.1-200k": "~82.3 (paper, average across tasks)",
    "Qwen2-7B-32k": "~65.2 (estimated)"
  },
  "notes": "评估时需要确保模型能够处理相应长度的上下文，推荐使用支持长上下文的推理框架（如vLLM with sliding window）。记录实际使用的上下文长度、位置编码方式和注意力机制类型。部分超长文本可能需要特殊的内存优化策略。"
}
