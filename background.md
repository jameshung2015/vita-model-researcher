# 评测指标与工具

本项目用于建立汽车 AI 与通用模型的评测指标体系、检测工具清单、检测系统与能力基线，对标产品与供应商能力。

工具需要解决的核心矛盾：
1. 模型的分类可以覆盖现有的商用与开源模型，且可横展、复用；
2. 模型与场景的分类可以覆盖现有的汽车场景，且可横展、复用；
3. 内部有自己的基准线可以评估模型与场景的能力；1）模型与场景关系。 2）现有上车的模型能力基线与指标。 3）新上的模型与供应商提供的模型可以量化评估。
4. 评价可以量化（公允的数据集合与自有的数据），可以在一个基线工具中完成评测。1）评测新模型。2）评测新场景。3）评测新供应商。

目标：四条核心矛盾（模型分类、场景分类、内部基线、可量化评测标准）

1. 建立通用且可扩展的模型分类体系
- 目标：设计一套层级化的模型分类标准，覆盖主流商用与开源模型（如大模型家族、微调/蒸馏模型、任务型小模型等），并支持横向扩展（引入新模型或供应商无需改动核心结构）。
- 验收标准：能覆盖 >=95% 市面主流模型（按流行度/上车模型列表），新增模型能在 <=2 人工小时内被分类到系统。

2. 建立通用且可复用的场景分类框架
- 目标：定义场景维度与标签体系（功能维度、风险等级、环境变量、输入模态等），覆盖汽车场景集合且可复用于新场景定义与组合测试。
- 验收标准：现有汽车场景 100% 被映射；新增场景能在 <=1 小时内被标注并生成对应测试标准。

3. 构建模型能力基线与对标机制
- 目标：为每类模型与场景定义能力度量矩阵（例如准确率、鲁棒性、延迟、资源消耗、多模态一致性、安全性指标等），并建立基线数据集与运行规范，支持模型/场景/供应商的横向对比。
- 验收标准：为每个主要场景至少有 1 个“基线模型”与 1 个“基线数据集”；对比报告能自动生成，包含关键指标与差距分析。

4. 实现可量化的一体化评测指标（建立基线工具）
- 目标：实现一个可复用的评测工具链操作说明（或指标说明），能够对新模型、新场景和供应商提供的自动化评测指标建议，从数据准备、指标计算到报告输出，且支持QA记录与审计。
- 验收标准：对一次完整评测（单模型×单场景）工具链从数据抽取研究到报告输出的总时长 <= 1 小时；输出包含标准化指标、置信区间与差距建议。

### 用户输入/输出契约
- 输入：用户可针对4个核心矛盾提供问题，提供用户一个知识库问答系统。
- 输出：使用智能体根据已有的数据或模型的推理提供输出建议，输出种类包含 1）现有知识问答。2）模型与场景的能力评估指标建议。

## 目标与可交付物

### 交付物
- 文档：模型分类规范、场景标签表、评测度量矩阵说明、操作手册
- 数据：基线数据集与数据规范
- 智能体：参照`agentToolrequirement.md` , 开发于`folder agents-toolchain`



### 关键风险与注意事项
- 数据合规：生产/车载数据涉及隐私与安全，必须在数据输入契约中强制声明并隔离敏感字段。
- 指标冲突：不同场景优先级不同（例如准确率 vs 延迟），需要在评测报告中显式权重或多维面板。
- 可扩展性成本：为了保证横向复用，初期设计需避免过度耦合特定模型格式或框架。
- 再现性：记录环境（硬件、依赖版本）与随机种子，确保评测结果可复现与审计。
- 维护成本：基线随时间变动，需明确基线更新策略（例如季度刷新或事件驱动）。

### 简洁执行清单（下一步）
1. 收集并列出当前“上车模型”与优先评测场景（输出：模型清单、场景清单）。
2. 起草并确认模型分类与场景标签（输出：两个规范文档）。
3. 定义首批基线数据集与度量矩阵（输出：数据集 + 指标脚本）。
4. 实现最小可用评测脚本（输入：模型、场景、数据；输出：JSON 报告 + 人类报告）。
5. 运行首轮评测并生成对比报告（输出：对比报告）。
6. 根据结果调整分类/基线并文档化更新策略。