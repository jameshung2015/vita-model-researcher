# 评测指标与工具

本项目用于建立汽车 AI 与通用模型的评测指标体系、检测工具清单、检测系统与能力基线，对标产品与供应商能力。

工具需要解决的核心矛盾：
1. 模型的分类可以覆盖现有的商用与开源模型，且可横展、复用；
2. 模型与场景的分类可以覆盖现有的汽车场景，且可横展、复用；
3. 内部有自己的基准线可以评估模型与场景的能力；1）模型与场景关系。 2）现有上车的模型能力基线与指标。 3）新上的模型与供应商提供的模型可以量化评估。
4. 评价可以量化（公允的数据集合与自有的数据），可以在一个基线工具中完成评测。1）评测新模型。2）评测新场景。3）评测新供应商。

目录结构（初版）：
- research/  — 研究计划与调研输出
- eval_system/    — 评估系统需求、架构与接口说明

目标：四条核心矛盾（模型分类、场景分类、内部基线、可量化评测）

1. 建立通用且可扩展的模型分类体系
- 目标：设计一套层级化的模型分类标准，覆盖主流商用与开源模型（如大模型家族、微调/蒸馏模型、任务型小模型等），并支持横向扩展（引入新模型或供应商无需改动核心结构）。
- 验收标准：能覆盖 >=95% 市面主流模型（按流行度/上车模型列表），新增模型能在 <=2 人工小时内被分类并上链到系统。

2. 建立通用且可复用的场景分类框架
- 目标：定义场景维度与标签体系（功能维度、风险等级、环境变量、输入模态等），覆盖汽车场景集合且可复用于新场景定义与组合测试。
- 验收标准：现有汽车场景（初始集合）100% 被映射；新增场景能在 <=1 小时内被标注并生成对应测试计划。

3. 构建内部能力基线与对标机制
- 目标：为每类模型与场景定义能力度量矩阵（例如准确率、鲁棒性、延迟、资源消耗、多模态一致性、安全性指标等），并建立基线数据集与运行规范，支持模型/场景/供应商的横向对比。
- 验收标准：为每个主要场景至少有 1 个“基线模型”与 1 个“基线数据集”；对比报告能自动生成，包含关键指标与差距分析。

4. 实现可量化的一体化评测流程（单一基线工具）
- 目标：实现一个可复用的评测工具链（或平台），能够对新模型、新场景和供应商提供端到端的自动化评测，从数据准备、指标计算到报告输出，且支持可复现与审计。
- 验收标准：对一次完整评测（单模型×单场景）工具链从数据准备到报告输出的总时长 <= 1 小时（依硬件可放宽）；输出包含标准化指标、置信区间与差距建议。

### 输入/输出契约
- 输入：模型包或访问描述（类型、权重或 API、版本、资源需求、作者/供应商）、场景定义（场景标签、输入模态样例、成功判定逻辑）、数据集（标注格式、元信息、隐私/合规声明）。
- 输出：标准化评测报告（指标数值、可视化表格、差距与建议）、可机器读取的评测结果（JSON/CSV，包含时间戳、版本、commit/id）、基线记录条目（用于历史对比）。

下一步：查看 `research/研究计划.md` 和 `eval_system/系统需求说明.md`。

## 目标与可交付物


### 可交付物（短清单）
- 文档：模型分类规范、场景标签表、评测度量矩阵说明、操作手册
- 数据：首批基线数据集与数据规范
- 工具：最小可用评测脚本/模块（含示例模型与场景），报告生成器（JSON + 人类可读）
- 报告：首轮对比评测报告（至少 3 个模型 × 3 个场景）


### 关键风险与注意事项
- 数据合规：生产/车载数据涉及隐私与安全，必须在数据输入契约中强制声明并隔离敏感字段。
- 指标冲突：不同场景优先级不同（例如准确率 vs 延迟），需要在评测报告中显式权重或多维面板。
- 可扩展性成本：为了保证横向复用，初期设计需避免过度耦合特定模型格式或框架。
- 再现性：记录环境（硬件、依赖版本）与随机种子，确保评测结果可复现与审计。
- 维护成本：基线随时间变动，需明确基线更新策略（例如季度刷新或事件驱动）。

### 简洁执行清单（下一步）
1. 收集并列出当前“上车模型”与优先评测场景（输出：模型清单、场景清单）。
2. 起草并确认模型分类与场景标签（输出：两个规范文档）。
3. 定义首批基线数据集与度量矩阵（输出：数据集 + 指标脚本）。
4. 实现最小可用评测脚本（输入：模型、场景、数据；输出：JSON 报告 + 人类报告）。
5. 运行首轮评测并生成对比报告（输出：对比报告）。
6. 根据结果调整分类/基线并文档化更新策略。