好的，根据您提供的 `background.md` 文件，我将为您设计一个分阶段的研究工具需求方案。该方案旨在支持您的汽车 AI 评测体系建设项目，并满足您提出的四个核心要求：分阶段、关键数据关联、多源采集验证、以及研究参照（URL）。

### 研究工具总体目标
该工具的核心定位是项目的“研究与知识管理中枢”，它将系统化地支持评测体系在建立过程中的信息采集、整理、验证和关联工作，为最终的 `eval_system`（评估系统）提供高质量的、可追溯的数据与决策依据。

---

### 第一阶段：基础调研与分类法定义 (Foundation Research & Taxonomy Development)

此阶段对应项目执行清单的步骤 1 和 2，目标是为模型和场景建立科学的分类标准。

**研究工具需求:**

1.  **多源情报聚合器 (Multi-Source Intelligence Aggregator)**
    *   **功能描述**: 工具需要内置一个强大的搜索引擎，能够同时查询并聚合来自多个来源的信息，包括但不限于：学术论文数据库 (如 arXiv, Google Scholar)、技术博客、开源社区 (如 Hugging Face, GitHub)、以及供应商官网。
    *   **关键关联数据**:
        *   `模型 (Model)` -> `供应商/作者 (Supplier/Author)`, `模型架构 (Architecture)`, `开源/商用 (License)`, `流行度指标 (Popularity Score)`
        *   `场景 (Scene)` -> `应用领域 (Domain)`, `车辆功能 (Vehicle Function)`, `相关法规 (Regulations)`
    *   **研究与验证**:
        *   提供并排比较(Side-by-Side)视图，用于对比不同来源对同一模型或场景的描述。
        *   所有采集条目必须自动记录来源 URL 和采集时间戳。
        *   提供“已验证” (Verified) 标签，需由研究人员手动确认信息准确性。

2.  **协同分类法构建器 (Collaborative Taxonomy Builder)**
    *   **功能描述**: 提供一个可视化的、支持多人协同编辑的界面，用于起草、迭代和最终确定模型与场景的分类标签体系。
    *   **关键关联数据**:
        *   `分类标签 (Tag)` -> `定义 (Definition)`, `父/子标签 (Parent/Child Tag)`, `关联模型/场景实例 (Linked Instances)`
    *   **研究与验证**:
        *   支持从聚合器中一键“添加为例”，将具体模型或场景作为标签定义的参考。
        *   每个标签的修改历史都应被记录，并可追溯到修改人与时间。
        *   提供版本控制，允许团队将某个版本的分类法保存为“基线规范 v1.0”。

### 第二阶段：评测维度与基线数据研究 (Metrics & Baseline Data Scoping)

此阶段对应项目执行清单的步骤 3，目标是定义评测指标矩阵和寻找合适的基线数据集。

**研究工具需求:**

1.  **公开基准搜索引擎 (Public Benchmark Search Engine)**
    *   **功能描述**: 专项优化搜索，用于发现和评估全球范围内的权威评测数据集和排行榜 (Leaderboards)。例如，能针对“汽车目标检测”场景，检索出 Waymo Open Dataset, nuScenes, KITTI 等，并提供排行榜的官方链接。
    *   **关键关联数据**:
        *   `数据集 (Dataset)` -> `发布机构 (Organization)`, `应用场景标签 (Scene Tags)`, `数据模态 (Modality)`, `许可证 (License)`, `数据量 (Size)`
        *   `排行榜 (Leaderboard)` -> `关联数据集 (Dataset)`, `Top模型 (Top Models)`, `关键指标 (Key Metrics)`
    *   **研究与验证**:
        *   工具应能解析排行榜页面，结构化地展示领先模型的性能指标。
        *   提供一键导入功能，将发现的数据集和指标添加至内部的“潜在基线库”。
        *   自动追踪关键数据集（如nuScenes）的更新动态，当有新版本或新挑战发布时提醒研究团队。

2.  **评测指标知识库 (Evaluation Metrics Knowledge Base)**
    *   **功能描述**: 建立一个内部维基（Wiki）系统，专门用于记录和解释各种AI评测指标。每个指标条目都应包含其定义、计算公式、适用场景、优缺点以及相关的学术论文或技术文档。
    *   **关键关联数据**:
        *   `指标 (Metric)` -> `定义 (Definition)`, `计算公式 (Formula)`, `适用模型/场景分类 (Applicable Categories)`, `参考来源 (Reference)`
    *   **研究与验证**:
        *   允许团队成员对指标的解释进行评论和修订，确保理解一致。
        *   通过关联功能，将第一阶段定义的“模型分类”和“场景分类”与指标进行链接，为后续自动生成“度量矩阵”打下基础。

### 第三阶段：评测工具链技术研究 (Toolchain & Implementation Research)

此阶段对应项目执行清单的步骤 4，目标是为“最小可用评测脚本”的实现提供技术选型支持。

**研究工具需求:**

1.  **开源评测框架扫描器 (Open-Source Evaluation Framework Scanner)**
    *   **功能描述**: 定向扫描 GitHub、技术社区等平台，发现并分析主流的 AI 模型评测框架和库（如 Hugging Face `evaluate`, `lm-evaluation-harness`, `MLPerf` 等）。工具应能自动提取框架的关键信息，如支持的模型类型、内置的评测指标、可扩展性、环境依赖等。
    *   **关键关联数据**:
        *   `框架/库 (Framework/Library)` -> `支持的模型格式 (Supported Formats)`, `内置指标 (Built-in Metrics)`, `社区活跃度 (Community Activity)`, `许可证 (License)`, `主要编程语言 (Language)`
    *   **研究与验证**:
        *   提供“一键部署”脚本或 Dockerfile 链接，方便团队快速试用。
        *   对不同框架进行特征对比，生成选型建议报告，例如：“框架A对大模型支持更好，框架B在资源消耗评测方面更成熟”。
        *   跟踪已选定框架的版本更新，当有重大功能发布或不兼容变更时发出通知。

2.  **可复现环境知识库 (Reproducible Environment Knowledge Base)**
    *   **功能描述**: 建立一个专门记录“如何确保评测结果可复现”的最佳实践库。内容包括但不限于：如何使用 Docker/Singularity 固化环境、如何管理 Python 依赖版本（如 poetry, pip-tools）、如何固定随机种子、以及如何记录硬件环境信息。
    *   **关键关联数据**:
        *   `技术实践 (Practice)` -> `适用场景 (Scenario)`, `优点 (Pros)`, `缺点 (Cons)`, `示例代码/配置 (Example Code)`
    *   **研究与验证**:
        *   提供标准化的环境记录模板（如 `environment.yaml`），团队在每次评测时可以基于此模板进行填写。
        *   收集并链接到相关的官方文档和高质量的技术博客，作为深度阅读的参考。

### 总结：研究工具的核心价值

这个研究工具并非要取代最终的评测系统，而是作为评测系统建设过程中的“脚手架”和“知识沉淀平台”。它的核心价值在于：

*   **结构化知识积累**：将研究过程中零散的信息（网页、论文、代码片段）转化为结构化的、相互关联的内部知识资产。
*   **提升研究效率**：通过专项搜索、信息聚合和自动化追踪，减少研究人员在信息检索和验证上花费的重复性劳动。
*   **保证决策质量**：为模型/场景分类的建立、评测维度的选取、技术框架的选型提供有数据支持、可追溯来源的决策依据。
*   **促进团队协作**：通过协同编辑和统一的知识库，确保团队成员对概念、标准和实践的理解保持一致，降低沟通成本。