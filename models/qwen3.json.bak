{
  "model_name": "Qwen3 (Qwen 3 series)",
  "variants": [
    {
      "name": "Qwen3-4B",
      "params": "~4B",
  "default_seq_len": 32768,
  "default_num_gpus": 1,
      "context_windows": ["32k"],
      "release_date": "2025-04-29 (Qwen3 release); updated 2025-08-06 for 2507 instruct/thinking variants",
      "model_card_link": "https://huggingface.co/Qwen/Qwen3-4B",
      "license": "Apache-2.0 (open weights per Qwen3 technical report)",
      "hardware_recommendations": {
        "recommended_deployment": "single GPU or CPU (small footprint)",
        "estimated_vram_fp16": "8-12 GB (estimate for inference / minimal batch)",
        "estimated_vram_int8": "4-6 GB (quantized, estimate)",
        "quantization_support": true,
        "suggested_quant_tools": ["GPTQ", "AWQ", "GGUF"],
        "notes": "Small dense model — quantization and CPU inference are practical. Validate exact VRAM via model card and local benchmark."
      }
    },
    {
      "name": "Qwen3-8B",
      "params": "~8.2B",
  "default_seq_len": 32768,
  "default_num_gpus": 2,
      "context_windows": ["32,768 (native)", "131,072 (with YaRN / extended runtimes)"],
      "release_date": "2025-04-29 (base release); modelcard updated Jul 2025",
      "model_card_link": "https://huggingface.co/Qwen/Qwen3-8B",
      "license": "Apache-2.0",
      "hardware_recommendations": {
        "recommended_deployment": "single high-memory GPU (preferred) or multi-GPU",
        "estimated_vram_fp16": "16-24 GB (estimate for full FP16 inference)",
        "estimated_vram_int8": "8-12 GB (quantized, estimate)",
        "quantization_support": true,
        "suggested_quant_tools": ["GPTQ", "AWQ", "vLLM quantization workflows"],
        "notes": "8B-class models often run on a single 24-48GB GPU; exact memory depends on context window and batch size."
      }
    },
    {
      "name": "Qwen3-30B-A3B",
      "params": "~30B total, ~3B activated (MoE)",
  "default_seq_len": 131072,
  "default_num_gpus": 4,
      "context_windows": ["128k (base)", "256k+ for 2507 variants"],
      "release_date": "2025-07-30 (Instruct-2507) / 2025-07-31 (Thinking-2507)",
      "model_card_link": "https://huggingface.co/Qwen/Qwen3-30B-A3B",
      "license": "Apache-2.0",
      "hardware_recommendations": {
        "recommended_deployment": "multi-GPU (model parallel) or cloud inference service; MoE-aware runtime required",
        "estimated_vram_fp16": "~48-80 GB (depends on activated experts and context length)",
        "estimated_vram_int8": "~24-40 GB (quantized, estimate; may require custom tool support)",
        "quantization_support": true,
        "suggested_quant_tools": ["GPTQ (dense parts)", "AWQ", "MoE-aware runtimes: SGLang, vLLM with MoE support"],
        "notes": "MoE models have lower activated-parameter footprints but require runtimes that support expert routing. Memory varies significantly with activation patterns and context length."
      }
    },
    {
      "name": "Qwen3-235B-A22B",
      "params": "~235B total, ~22B activated (MoE)",
  "default_seq_len": 131072,
  "default_num_gpus": 8,
      "context_windows": ["128k (base)", "256k for 2507 variants", "up to 1M tokens in updated 2507 experimental support"],
      "release_date": "2025-07-21 (Instruct-2507) / 2025-07-25-31 (Thinking/Instruct updates)",
      "model_card_link": "https://huggingface.co/Qwen/Qwen3-235B-A22B",
      "license": "Apache-2.0",
      "hardware_recommendations": {
        "recommended_deployment": "multi-node multi-GPU clusters (model & tensor parallelism) or cloud-hosted inference; specialized MoE runtime",
        "estimated_vram_fp16": "~400-800+ GB total (depends on tensor/model parallelism and offloading strategies)",
        "estimated_vram_int8": "~150-300 GB (quantized, estimate using advanced GPTQ/AWQ pipelines and offloading)",
        "quantization_support": true,
        "suggested_quant_tools": ["GPTQ (research-grade)", "AWQ (efficient int8)", "custom MoE-aware quant pipelines"],
        "notes": "Large MoE model: memory estimates are highly dependent on activated experts, TP/PP strategy, and context length. Use cloud providers (A100 80GB, H100) with model parallel tooling or hosted APIs. Always validate with modelcard and small pilot runs."
      }
    }
  ],
  "input_types": ["text"],
  "output_types": ["text"],
  "capabilities": {
    "languages_supported": "119+",
    "long_context_support": ["40k","256k"],
    "multimodal": "Qwen ecosystem provides image/video understanding in product (Qwen Chat); core Qwen3 focuses on text but integrated multimodal endpoints exist",
    "reasoning_and_code": "Published benchmarks report strong performance on coding, math, and commonsense reasoning"
  },
  "architecture_family": "Transformer-based; mix of dense and MoE (Mixture-of-Experts) variants reported",
  "size_params": {
    "notable_variants": ["4B","8B","30B","235B"],
    "activated_params_note": "Some MoE variants report a smaller base + activated experts (e.g., 30B-A3B, 235B-A22B)"
  },
  "license": "See upstream sources (model cards on Hugging Face / QwenLM GitHub) — licensing varies by release/variant",
  "deploy_requirements": {
    "small_variant": "Consumer-grade GPU or CPU (4B/8B) for inference; smaller memory footprint",
    "large_variant": "Multi-GPU or cloud inference (235B) — recommended NVidia A100/RTX 6000 series or cloud instances; MoE models require specialist runtime support",
    "context_window": "Supports very long context lengths (40k and some variants up to 256k tokens) — requires memory/architecture that supports long-context attention"
  },
  "notes": "Qwen3 is Alibaba's third-generation Qwen family with both dense and MoE variants, targeting improved reasoning and scalable long-context performance. Post-trained 'Instruct' or 'Thinking' variants are available (e.g., -Instruct-2507, -Thinking-2507). Availability on Hugging Face, Ollama, and official Qwen pages.",
  "sources": [
    "https://qwenlm.github.io/blog/qwen3/",
    "https://github.com/QwenLM/Qwen3",
    "https://huggingface.co/Qwen/Qwen3-8B",
    "https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f",
    "https://arxiv.org/abs/2505.09388",
    "https://ollama.com/library/qwen3",
    "https://chat.qwen.ai/"
  ]
}
