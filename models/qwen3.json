{
  "model_name": "Qwen3",
  "homepage": "https://qwenlm.github.io/blog/qwen3/",
  "model_card_link": "https://huggingface.co/Qwen",
  "license": "Apache-2.0 (per model cards)",
  "provenance": {
    "provider": "QwenLM",
    "release_date": "2025-04-29",
    "notes": "Family release; later 2507 variants updated in July 2025"
  },
  "input_types": ["text"],
  "output_types": ["text"],
  "architecture_family": "decoder-only",
  "architecture_details": "Dense and MoE variants in the family",
  "size_params": {
    "notable_variants": ["4B","8B","30B","235B"],
    "activated_params_note": "MoE variants report activated params (e.g., 30B-A3B, 235B-A22B)"
  },
  "capabilities": {
    "languages_supported": "100+",
    "long_context_support": ["40k","256k"],
    "reasoning_and_code": "Strong coding/math/common-sense benchmarks"
  },
  "variants": [
    {
      "name": "Qwen3-4B",
      "params": 4,
      "context_windows": [32768],
      "default_seq_len": 32768,
      "default_num_gpus": 1,
      "model_card_link": "https://huggingface.co/Qwen/Qwen3-4B",
      "release_date": "2025-04-29",
      "hardware_recommendations": {
        "fp16_gb_per_gpu": 10,
        "int8_gb_per_gpu": 6,
        "recommended_num_gpus": 1,
        "notes": "Small dense model; CPU or single-GPU feasible"
      }
    },
    {
      "name": "Qwen3-8B",
      "params": 8.2,
      "context_windows": [32768, 131072],
      "default_seq_len": 32768,
      "default_num_gpus": 1,
      "model_card_link": "https://huggingface.co/Qwen/Qwen3-8B",
      "release_date": "2025-04-29",
      "hardware_recommendations": {
        "fp16_gb_per_gpu": 20,
        "int8_gb_per_gpu": 12,
        "recommended_num_gpus": 1,
        "notes": "Runs on a single 24â€“48GB GPU depending on context/batch"
      }
    },
    {
      "name": "Qwen3-30B-A3B",
      "params": 30,
      "context_windows": [131072, 256000],
      "default_seq_len": 131072,
      "default_num_gpus": 4,
      "model_card_link": "https://huggingface.co/Qwen/Qwen3-30B-A3B",
      "release_date": "2025-07-30",
      "hardware_recommendations": {
        "fp16_gb_per_gpu": 24,
        "int8_gb_per_gpu": 16,
        "recommended_num_gpus": 4,
        "notes": "MoE; memory depends on active experts and context"
      }
    },
    {
      "name": "Qwen3-235B-A22B",
      "params": 235,
      "context_windows": [131072, 256000],
      "default_seq_len": 131072,
      "default_num_gpus": 8,
      "model_card_link": "https://huggingface.co/Qwen/Qwen3-235B-A22B",
      "release_date": "2025-07-21",
      "hardware_recommendations": {
        "fp16_gb_per_gpu": 80,
        "int8_gb_per_gpu": 40,
        "recommended_num_gpus": 8,
        "notes": "Large MoE; requires distributed inference runtimes"
      }
    }
  ],
  "evaluation": {
    "papers": ["https://arxiv.org/abs/2505.09388"],
    "benchmarks": ["lmarena", "chatbot_arena", "mmlu", "gsm8k"],
    "model_cards": [
      "https://huggingface.co/Qwen/Qwen3-4B",
      "https://huggingface.co/Qwen/Qwen3-8B",
      "https://huggingface.co/Qwen/Qwen3-30B-A3B",
      "https://huggingface.co/Qwen/Qwen3-235B-A22B"
    ]
  },
  "inference": {
    "latency_ms": null,
    "latency_level": "medium",
    "throughput_rps": null,
    "concurrency": null,
    "memory_gb": null,
    "quantization_friendly": true,
    "supported_hardware": ["A100","H100","RTX 6000","CPU"],
    "distributed_support": true,
    "notes": "Use scripts/estimate_vram.py for heuristic sizing of Qwen3-8B / long context"
  },
  "notes": "Qwen3 is the third-generation Qwen family with dense and MoE variants."
}
