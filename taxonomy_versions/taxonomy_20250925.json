{
  "version": "20250925",
  "categories": {
    "inputs": [
      "audio",
      "document",
      "image",
      "text",
      "video"
    ],
    "outputs": [
      "text"
    ],
    "architecture": [
      "decoder-only",
      "encoder-decoder",
      "mixture-of-experts",
      "moe multimodal (encoder vision + moe decoder)",
      "multimodal",
      "transformer encoder-decoder (audio-to-text)"
    ],
    "size": [
      "{'notable_variants': ['4b', '8b', '30b', '235b'], 'activated_params_note': 'moe variants report activated params (e.g., 30b-a3b, 235b-a22b)'}",
      "{'notable_variants': ['deepseek-v3', 'deepseek-r1'], 'activated_params_note': 'deepseek-v3 activates ~37b params per token; trained on ~14.8t tokens (report).'}",
      "{'notable_variants': ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-2.0-flash'], 'activated_params_note': 'parameter counts not publicly disclosed; api-managed deployment.'}",
      "{'notable_variants': ['gpt-oss-120b', 'gpt-oss-20b'], 'activated_params_note': 'readme states 120b has ~5.1b active params (117b total); 20b has ~3.6b active params (21b total).'}"
    ],
    "tags": [
      "api",
      "asr",
      "closed-weight",
      "commercial-use",
      "cot",
      "decoder-only",
      "fp8",
      "harmony-format",
      "long-context",
      "mla",
      "moe",
      "moonvit",
      "multilingual",
      "multimodal",
      "native-resolution",
      "open-source",
      "open-weight",
      "quantization:mxfp4",
      "rl",
      "speech-to-text",
      "vision-language"
    ]
  },
  "mappings": [
    {
      "model_ref": "Baichuan-Omni",
      "inputs": [
        "text"
      ],
      "outputs": [
        "text"
      ],
      "arch": "decoder-only",
      "size": null,
      "tags": []
    },
    {
      "model_ref": "DeepSeek",
      "inputs": [
        "text"
      ],
      "outputs": [
        "text"
      ],
      "arch": "mixture-of-experts",
      "size": "{'notable_variants': ['deepseek-v3', 'deepseek-r1'], 'activated_params_note': 'deepseek-v3 activates ~37b params per token; trained on ~14.8t tokens (report).'}",
      "tags": [
        "moe",
        "mla",
        "fp8",
        "rl",
        "cot",
        "open-weight",
        "commercial-use"
      ]
    },
    {
      "model_ref": "Gemini",
      "inputs": [
        "text",
        "image",
        "audio",
        "video"
      ],
      "outputs": [
        "text"
      ],
      "arch": "multimodal",
      "size": "{'notable_variants': ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-2.0-flash'], 'activated_params_note': 'parameter counts not publicly disclosed; api-managed deployment.'}",
      "tags": [
        "multimodal",
        "api",
        "closed-weight",
        "long-context"
      ]
    },
    {
      "model_ref": "gpt-4o",
      "inputs": [
        "text"
      ],
      "outputs": [
        "text"
      ],
      "arch": "decoder-only",
      "size": null,
      "tags": []
    },
    {
      "model_ref": "gpt-5o",
      "inputs": [
        "text"
      ],
      "outputs": [
        "text"
      ],
      "arch": "decoder-only",
      "size": null,
      "tags": []
    },
    {
      "model_ref": "gpt-oss",
      "inputs": [
        "text"
      ],
      "outputs": [
        "text"
      ],
      "arch": "decoder-only",
      "size": "{'notable_variants': ['gpt-oss-120b', 'gpt-oss-20b'], 'activated_params_note': 'readme states 120b has ~5.1b active params (117b total); 20b has ~3.6b active params (21b total).'}",
      "tags": [
        "open-weight",
        "moe",
        "decoder-only",
        "harmony-format",
        "quantization:mxfp4"
      ]
    },
    {
      "model_ref": "Kimi-Audio",
      "inputs": [
        "audio"
      ],
      "outputs": [
        "text"
      ],
      "arch": "encoder-decoder",
      "size": null,
      "tags": []
    },
    {
      "model_ref": "Kimi-VL",
      "inputs": [
        "text",
        "image",
        "video",
        "document"
      ],
      "outputs": [
        "text"
      ],
      "arch": "moe multimodal (encoder vision + moe decoder)",
      "size": null,
      "tags": [
        "vision-language",
        "moe",
        "long-context",
        "open-source",
        "moonvit",
        "native-resolution"
      ]
    },
    {
      "model_ref": "Qwen2.5-Omni",
      "inputs": [
        "text"
      ],
      "outputs": [
        "text"
      ],
      "arch": "decoder-only",
      "size": null,
      "tags": []
    },
    {
      "model_ref": "Qwen3",
      "inputs": [
        "text"
      ],
      "outputs": [
        "text"
      ],
      "arch": "decoder-only",
      "size": "{'notable_variants': ['4b', '8b', '30b', '235b'], 'activated_params_note': 'moe variants report activated params (e.g., 30b-a3b, 235b-a22b)'}",
      "tags": []
    },
    {
      "model_ref": "STEP-Audio-AQAA",
      "inputs": [
        "audio"
      ],
      "outputs": [
        "text"
      ],
      "arch": "encoder-decoder",
      "size": null,
      "tags": []
    },
    {
      "model_ref": "Whisper ASR",
      "inputs": [
        "audio"
      ],
      "outputs": [
        "text"
      ],
      "arch": "transformer encoder-decoder (audio-to-text)",
      "size": null,
      "tags": [
        "asr",
        "speech-to-text",
        "multilingual",
        "open-source"
      ]
    }
  ],
  "source": "models_index"
}